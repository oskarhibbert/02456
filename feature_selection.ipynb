{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b76384-8c65-46d4-8b9a-c09c055fe981",
   "metadata": {},
   "source": [
    "## DOCUMENT PREAMBLE\n",
    "\n",
    "To run this document in Google Colab, please upload `ebnerd_small.zip` and `small_train_users_df_expanded.parquet` to the available Files for this document.\n",
    "\n",
    "To run this document locally, please see the instructions in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94b1ac3-6010-4700-a786-ce1420da6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Configure tqdm and matplotlib\n",
    "tqdm.pandas()\n",
    "plt.style.use(\"classic\")\n",
    "#plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8ca8d8-7d43-42ea-9cd4-a5bcd3288ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from parquet files\n",
    "def load_data(version, data_type):\n",
    "    if data_type not in [\"train\", \"validation\"]:\n",
    "        raise ValueError(\"data_type must be either 'train' or 'validation'\")\n",
    "\n",
    "    # Read parquet files into DataFrames\n",
    "    behaviors_df = pd.read_parquet(\n",
    "        f\"./data/ebnerd_{version}/{data_type}/behaviors.parquet\"\n",
    "    )\n",
    "    history_df = pd.read_parquet(f\"./data/ebnerd_{version}/{data_type}/history.parquet\")\n",
    "    articles_df = pd.read_parquet(f\"./data/ebnerd_{version}/articles.parquet\")\n",
    "\n",
    "    # Print DataFrame info\n",
    "    for name, df in zip(\n",
    "        [f\"{data_type}/behaviors\", f\"{data_type}/history\", \"articles\"],\n",
    "        [behaviors_df, history_df, articles_df],\n",
    "    ):\n",
    "        print(f\"--- '{name}' ---\\n\")\n",
    "        print(df.info(), \"\\n\")\n",
    "\n",
    "    return behaviors_df, history_df, articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474f56f-0984-4d00-8403-9c6188174d2b",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5344a406-3f3c-41a0-944d-02e9804ba406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (2254478, 62)\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "users_df = pd.read_parquet(f\"./data_processed/small_train_users_df_expanded.parquet\")\n",
    "behaviors_df = pd.read_parquet(f\"./data_processed/small_train_behaviors_df_expanded.parquet\")\n",
    "articles_df = pd.read_parquet(f\"./data_processed/small_train_articles_df_expanded.parquet\")\n",
    "\n",
    "# Merge datasets\n",
    "data = pd.merge(behaviors_df, users_df, on='user_id', how='left')\n",
    "data = pd.merge(data, articles_df, left_on='impression_article_id', right_on='article_id', how='left')\n",
    "\n",
    "# Display shape for debugging\n",
    "print(f\"Data Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebe2de-2a2a-4327-ab80-18a96b5e5ce0",
   "metadata": {},
   "source": [
    "### IDENTIFY NON-NUMERIC COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c8858a-87b6-4705-a15c-34262c963bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-Numeric Columns (object type):\n",
      "['user_top_categories', 'user_top_subcategories', 'user_top_topics', 'user_top_ner_clusters', 'article_title', 'article_subtitle', 'article_is_premium', 'article_body', 'article_type', 'article_url', 'article_ner_clusters', 'article_entity_groups', 'article_topics', 'article_subcategory', 'article_category_str', 'article_sentiment_label']\n",
      "\n",
      "Array-Like Columns (list or ndarray):\n",
      "['user_top_categories', 'user_top_subcategories', 'user_top_topics', 'user_top_ner_clusters', 'article_ner_clusters', 'article_entity_groups', 'article_topics', 'article_subcategory']\n",
      "Error processing column user_top_categories: unhashable type: 'numpy.ndarray'\n",
      "Error processing column user_top_subcategories: unhashable type: 'numpy.ndarray'\n",
      "Error processing column user_top_topics: unhashable type: 'numpy.ndarray'\n",
      "Error processing column user_top_ner_clusters: unhashable type: 'numpy.ndarray'\n",
      "Column: article_title, Unique Values: 1642\n",
      "Column: article_subtitle, Unique Values: 1614\n",
      "Column: article_is_premium, Unique Values: 2\n",
      "Column: article_body, Unique Values: 1638\n",
      "Column: article_type, Unique Values: 5\n",
      "Column: article_url, Unique Values: 1642\n",
      "Error processing column article_ner_clusters: unhashable type: 'numpy.ndarray'\n",
      "Error processing column article_entity_groups: unhashable type: 'numpy.ndarray'\n",
      "Error processing column article_topics: unhashable type: 'numpy.ndarray'\n",
      "Error processing column article_subcategory: unhashable type: 'numpy.ndarray'\n",
      "Column: article_category_str, Unique Values: 15\n",
      "Column: article_sentiment_label, Unique Values: 3\n",
      "Column: user_top_categories, Example Value: ['nyheder' 'sport' 'underholdning']\n",
      "Column: user_top_subcategories, Example Value: [133 196 432]\n",
      "Column: user_top_topics, Example Value: ['Kendt' 'Sport' 'Begivenhed']\n",
      "Column: user_top_ner_clusters, Example Value: ['Ekstra Bladet' 'Danmark' 'Twitter']\n",
      "Column: article_ner_clusters, Example Value: ['Ben' 'Blæstegnen' 'Engletårer' 'Jelling Musikfestival' 'Okay'\n",
      " 'Soundvenue' 'Tessa' 'Tessa hævn' 'Theresa Ann Fallesen']\n",
      "Column: article_entity_groups, Example Value: ['PROD' 'PROD' 'PROD' 'EVENT' 'PROD' 'ORG' 'PER' 'PROD' 'PER']\n",
      "Column: article_topics, Example Value: ['Kendt' 'Underholdning' 'Musik og lyd']\n",
      "Column: article_subcategory, Example Value: [500]\n",
      "Shape after dropping array-like columns: (2254478, 54)\n"
     ]
    }
   ],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "array_like_columns = [col for col in data.columns if isinstance(data[col].iloc[0], (list, np.ndarray))]\n",
    "\n",
    "print(f\"\\nNon-Numeric Columns (object type):\\n{list(non_numeric_columns)}\")\n",
    "print(f\"\\nArray-Like Columns (list or ndarray):\\n{array_like_columns}\")\n",
    "\n",
    "# Analyze unique values for non-numeric columns\n",
    "for col in non_numeric_columns:\n",
    "    try:\n",
    "        unique_count = data[col].nunique()\n",
    "        print(f\"Column: {col}, Unique Values: {unique_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing column {col}: {e}\")\n",
    "\n",
    "# For array-like columns, handle them separately or drop them\n",
    "for col in array_like_columns:\n",
    "    print(f\"Column: {col}, Example Value: {data[col].iloc[0]}\")\n",
    "\n",
    "# Drop array-like columns (if not needed)\n",
    "data = data.drop(columns=array_like_columns, errors='ignore')\n",
    "print(f\"Shape after dropping array-like columns: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b805b-35c2-4d91-a2bb-2359c8e111a6",
   "metadata": {},
   "source": [
    "### DROP HIGH CARDINALITY COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2cf99e-1b52-483c-8268-44f93b309278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: article_title, Unique Values: 1642\n",
      "Column: article_subtitle, Unique Values: 1614\n",
      "Column: article_is_premium, Unique Values: 2\n",
      "Column: article_body, Unique Values: 1638\n",
      "Column: article_type, Unique Values: 5\n",
      "Column: article_url, Unique Values: 1642\n",
      "Column: article_category_str, Unique Values: 15\n",
      "Column: article_sentiment_label, Unique Values: 3\n",
      "\n",
      "Columns to Drop (High Cardinality or Array-Like): ['article_title', 'article_subtitle', 'article_body', 'article_url']\n",
      "Shape after dropping high cardinality and array-like columns: (2254478, 50)\n"
     ]
    }
   ],
   "source": [
    "# Threshold for dropping columns with high cardinality\n",
    "high_cardinality_threshold = 50  # Adjust as necessary\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "array_like_columns = [col for col in data.columns if isinstance(data[col].iloc[0], (list, np.ndarray))]\n",
    "\n",
    "# Analyze cardinality for non-numeric columns\n",
    "high_cardinality_columns = []\n",
    "for col in non_numeric_columns:\n",
    "    unique_count = data[col].nunique()\n",
    "    print(f\"Column: {col}, Unique Values: {unique_count}\")\n",
    "    if unique_count > high_cardinality_threshold:\n",
    "        high_cardinality_columns.append(col)\n",
    "\n",
    "# Drop high-cardinality columns\n",
    "columns_to_drop = high_cardinality_columns + array_like_columns\n",
    "print(f\"\\nColumns to Drop (High Cardinality or Array-Like): {columns_to_drop}\")\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Display the resulting dataset shape\n",
    "print(f\"Shape after dropping high cardinality and array-like columns: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979ed12-19e2-4440-81b5-c8f64c07fd84",
   "metadata": {},
   "source": [
    "### DEFINE COLUMNS TO ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdd51c4-3d4d-4ab5-8d44-ab08858868a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding relevant columns: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after encoding relevant columns: (2254478, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define columns to encode\n",
    "columns_to_encode = ['article_is_premium', 'article_type', 'article_category_str', 'article_sentiment_label']\n",
    "\n",
    "# Apply label encoding\n",
    "label_encoders = {}\n",
    "for col in tqdm(columns_to_encode, desc=\"Encoding relevant columns\"):\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le  # Store encoders for inverse transforms later\n",
    "\n",
    "print(f\"Shape after encoding relevant columns: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c64f11-795d-461d-9569-efa0d093f442",
   "metadata": {},
   "source": [
    "### SELECT FEATURES AND TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b00d8c2-9a58-4c7e-ab03-c59265fe7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (45): ['impression_article_id', 'impression_time', 'impression_read_time', 'impression_scroll_percentage', 'impression_device_type', 'impression_article_id_inview', 'impression_article_id_clicked', 'user_is_sso', 'user_is_subscriber', 'impression_next_read_time', 'impression_next_scroll_percentage', 'impression_day_of_week', 'impression_hour', 'impression_day_of_week_sin', 'impression_day_of_week_cos', 'impression_hour_sin', 'impression_hour_cos', 'impression_is_frontpage', 'session_avg_read_time', 'session_avg_scroll_percentage', 'user_avg_sentiment_score', 'user_total_premium_viewed', 'user_avg_scroll_percentage', 'user_avg_read_time', 'user_total_articles_viewed', 'article_is_premium', 'article_published_time', 'article_type', 'article_category', 'article_category_str', 'article_sentiment_score', 'article_sentiment_label', 'article_published_year', 'article_published_month', 'article_published_day', 'article_published_day_of_week', 'article_published_hour', 'article_published_day_of_week_sin', 'article_published_day_of_week_cos', 'article_published_hour_sin', 'article_published_hour_cos', 'article_published_season', 'article_total_inviews_log', 'article_total_pageviews_log', 'article_total_read_time_log']\n",
      "Feature matrix shape: (2254478, 45)\n",
      "Target shape: (2254478,)\n",
      "Non-Numeric Columns in Features (X): []\n",
      "Shape after dropping non-numeric columns: (2254478, 45)\n"
     ]
    }
   ],
   "source": [
    "# Select Features and Target\n",
    "\n",
    "# List of identifier columns to exclude\n",
    "columns_to_exclude = ['user_id', 'impression_id', 'article_id', 'impression_session_id']\n",
    "\n",
    "# Automatically select all remaining feature columns except for identifier columns and target\n",
    "target_column = 'target'\n",
    "feature_columns = [col for col in data.columns if col not in columns_to_exclude + [target_column]]\n",
    "\n",
    "print(f\"Selected Features ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "# Display shapes to confirm successful split\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Check if there are any non-numeric columns in X\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Non-Numeric Columns in Features (X): {list(non_numeric_columns)}\")\n",
    "\n",
    "# Drop non-numeric columns (optional, only if needed)\n",
    "X = X.drop(columns=non_numeric_columns, errors='ignore')\n",
    "print(f\"Shape after dropping non-numeric columns: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac6206-f33e-4800-af73-302b64f3ff70",
   "metadata": {},
   "source": [
    "### HANDLE MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8bcc96b-be6b-4e2e-a99d-b532cdda2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime Columns: ['impression_time', 'article_published_time']\n",
      "Total Missing Values After Imputation: 0\n",
      "Shape after imputation: (2254478, 51)\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify datetime columns and convert them to numeric timestamps\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "print(f\"Datetime Columns: {list(datetime_columns)}\")\n",
    "\n",
    "# Option 2: Convert datetime columns to timestamps and feature-engineered columns\n",
    "for col in datetime_columns:\n",
    "    X[f'{col}_timestamp'] = pd.to_datetime(X[col]).astype(int) // 10**9  # Unix timestamp\n",
    "    X[f'{col}_year'] = pd.to_datetime(X[col]).dt.year\n",
    "    X[f'{col}_month'] = pd.to_datetime(X[col]).dt.month\n",
    "    X[f'{col}_day'] = pd.to_datetime(X[col]).dt.day\n",
    "\n",
    "# Drop original datetime columns if not needed anymore\n",
    "X = X.drop(columns=datetime_columns, errors='ignore')\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_values_after_imputation = X_imputed.isnull().sum().sum()\n",
    "print(f\"Total Missing Values After Imputation: {missing_values_after_imputation}\")\n",
    "\n",
    "# Display the shape of X after imputation\n",
    "print(f\"Shape after imputation: {X_imputed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbabe2-79cc-4895-ad04-c6f8cdef0f96",
   "metadata": {},
   "source": [
    "### SCALE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ea67d3-a75b-4d4c-a76f-f3979ee0125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after scaling: (2254478, 51)\n",
      "Feature Means After Scaling: \n",
      "impression_article_id          -1.296179e-15\n",
      "impression_read_time           -6.000828e-18\n",
      "impression_scroll_percentage   -1.208864e-15\n",
      "impression_device_type         -1.132656e-16\n",
      "impression_article_id_inview    3.834352e-17\n",
      "dtype: float64\n",
      "Feature Std Devs After Scaling: \n",
      "impression_article_id           1.0\n",
      "impression_read_time            1.0\n",
      "impression_scroll_percentage    1.0\n",
      "impression_device_type          1.0\n",
      "impression_article_id_inview    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Scale Features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features in X\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X_imputed.columns)\n",
    "\n",
    "# Display the shape of X after scaling\n",
    "print(f\"Shape after scaling: {X_scaled.shape}\")\n",
    "\n",
    "# Check statistics to confirm scaling (mean should be ~0, std should be ~1)\n",
    "print(f\"Feature Means After Scaling: \\n{X_scaled.mean().head()}\")\n",
    "print(f\"Feature Std Devs After Scaling: \\n{X_scaled.std().head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf3c69-2f91-4c8c-8506-0e6b7ce1be21",
   "metadata": {},
   "source": [
    "## TESTS FOR FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c5677-6b6b-40de-97e8-95eaec06fa0f",
   "metadata": {},
   "source": [
    "### MUTUAL INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cc0ee-a2f2-4486-9458-36ba8945af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Mutual Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Mutual Information:  10%|▉         | 5/51 [02:39<24:50, 32.39s/it]"
     ]
    }
   ],
   "source": [
    "# Calculate Mutual Information (with progress bar and index-based approach)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom function to track progress for mutual_info_classif\n",
    "def calculate_mutual_info(X, y):\n",
    "    mi_scores = []\n",
    "    for i in tqdm(range(X.shape[1]), desc=\"Calculating Mutual Information\"):\n",
    "        mi_score = mutual_info_classif(X.iloc[:, [i]], y, discrete_features=False)[0]\n",
    "        mi_scores.append(mi_score)\n",
    "    return np.array(mi_scores)\n",
    "\n",
    "# Calculate Mutual Information\n",
    "print(\"\\nCalculating Mutual Information...\")\n",
    "mi_scores = calculate_mutual_info(X_scaled, y)\n",
    "mi_scores = pd.Series(mi_scores, index=range(X_scaled.shape[1])).sort_values(ascending=False)  # Use indices instead of names\n",
    "\n",
    "# Display top 10 features by Mutual Information\n",
    "print(\"\\nTop 10 Feature Indices by Mutual Information:\\n\", mi_scores.head(10))\n",
    "\n",
    "# Store indices of top 15 features\n",
    "top_mi_indices = mi_scores.head(15).index\n",
    "print(f\"Top 15 Feature Indices by Mutual Information: {list(top_mi_indices)}\")\n",
    "\n",
    "# Display full results\n",
    "feature_scores_mi = pd.DataFrame({\n",
    "    'Feature Index': range(X_scaled.shape[1]),\n",
    "    'Mutual Info': mi_scores.values\n",
    "})\n",
    "from IPython.display import display\n",
    "display(feature_scores_mi) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa25f36-8442-4211-95ac-7ba1a09d2181",
   "metadata": {},
   "source": [
    "### RANDOM FOREST FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578ede3-4576-4d0b-a618-12fa66083b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Random Forest Feature Importance (with progress bar and index-based approach)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Custom function to track the Random Forest fitting process\n",
    "class RandomForestWithProgress(RandomForestClassifier):\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\nFitting Random Forest for feature importance...\")\n",
    "        for i in tqdm(range(1, 2), desc=\"Random Forest Training\"):  # Simulate progress\n",
    "            super().fit(X, y)\n",
    "        return self\n",
    "\n",
    "# Train Random Forest and calculate feature importance\n",
    "rf_model = RandomForestWithProgress(random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "rf_feature_importance = pd.Series(rf_model.feature_importances_, index=range(X_scaled.shape[1])).sort_values(ascending=False)\n",
    "\n",
    "# Display top 10 features by Random Forest Importance\n",
    "print(\"\\nTop 10 Feature Indices by Random Forest Importance:\\n\", rf_feature_importance.head(10))\n",
    "\n",
    "# Store indices of top 15 features\n",
    "top_rf_indices = rf_feature_importance.head(15).index\n",
    "print(f\"Top 15 Feature Indices by Random Forest Importance: {list(top_rf_indices)}\")\n",
    "\n",
    "# Display full results\n",
    "feature_scores_rf = pd.DataFrame({\n",
    "    'Feature Index': range(X_scaled.shape[1]),\n",
    "    'RF Importance': rf_feature_importance.values\n",
    "})\n",
    "from IPython.display import display\n",
    "display(feature_scores_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4975f17e-c19d-4f47-ae73-f5dbd9c60fad",
   "metadata": {},
   "source": [
    "### RECURSIVE FEATURE ELIMINATION (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffd103-9aa0-4907-b51e-0cbb9cca1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Recursive Feature Elimination (RFE) with Progress Bar\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # For DataFrame display\n",
    "from IPython.display import display  # Alternative to ace_tools\n",
    "\n",
    "# Custom RFE class to add progress bar\n",
    "class RFEWithProgress(RFE):\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\nPerforming Recursive Feature Elimination (this may take a while)...\")\n",
    "        for i in tqdm(range(1, 2), desc=\"RFE Progress\"):  # Simulate progress\n",
    "            super().fit(X, y)\n",
    "        return self\n",
    "\n",
    "# Perform RFE to select the top 10 features\n",
    "rfe = RFEWithProgress(estimator=RandomForestClassifier(random_state=42, n_jobs=-1), n_features_to_select=10)\n",
    "rfe.fit(X_scaled, y)\n",
    "\n",
    "rfe_support = pd.Series(rfe.support_, index=X_scaled.columns)\n",
    "selected_features_rfe = rfe_support[rfe_support == True].index\n",
    "\n",
    "# Display selected features\n",
    "print(\"\\nTop 10 Features Selected by RFE:\\n\", list(selected_features_rfe))\n",
    "\n",
    "# Display full results for all features\n",
    "feature_scores_rfe = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'RFE Support': rfe_support.values\n",
    "})\n",
    "\n",
    "# Display DataFrame directly using IPython display\n",
    "display(feature_scores_rfe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c062648-07bc-4d51-8b1c-bf9d77885184",
   "metadata": {},
   "source": [
    "### COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021ce4d-a666-46f8-9f90-8cf6848e95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Combine and Display Feature Selection Result\n",
    "from IPython.display import display  # Use display to show DataFrame\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "feature_scores_combined = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'Mutual Info': mi_scores.values,\n",
    "    'RF Importance': rf_feature_importance.values,\n",
    "#   'RFE Support': rfe_support.values\n",
    "})\n",
    "\n",
    "# Sort by Random Forest Importance (or you can choose another sorting metric)\n",
    "feature_scores_combined = feature_scores_combined.sort_values(by='RF Importance', ascending=False)\n",
    "\n",
    "# Display the top 10 features from the combined results\n",
    "print(\"\\nTop 10 Features by Combined Results:\\n\")\n",
    "display(feature_scores_combined.head(10))\n",
    "\n",
    "# Display the entire feature score table\n",
    "display(feature_scores_combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
