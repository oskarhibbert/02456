{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "## DOCUMENT PREAMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configure matplotlib\n",
    "# tqdm.pandas()\n",
    "plt.style.use(\"classic\")\n",
    "#plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set document parameters\n",
    "data_version = \"demo\"\n",
    "data_type = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from parquet files\n",
    "def load_data(data_version, data_type, print_info=False):\n",
    "    if data_type not in [\"train\", \"validation\"]:\n",
    "        raise ValueError(\"data_type must be either 'train' or 'validation'\")\n",
    "\n",
    "    # Read parquet files into DataFrames\n",
    "    behaviors_df = pd.read_parquet(\n",
    "        f\"./data/ebnerd_{data_version}/{data_type}/behaviors.parquet\"\n",
    "    )\n",
    "    history_df = pd.read_parquet(f\"./data/ebnerd_{data_version}/{data_type}/history.parquet\")\n",
    "    articles_df = pd.read_parquet(f\"./data/ebnerd_{data_version}/articles.parquet\")\n",
    "\n",
    "    # Print DataFrame info\n",
    "    if print_info:\n",
    "        for name, df in zip(\n",
    "            [f\"{data_type}/behaviors\", f\"{data_type}/history\", \"articles\"],\n",
    "            [behaviors_df, history_df, articles_df],\n",
    "        ):\n",
    "            print(f\"--- '{name}' ---\\n\")\n",
    "            print(df.info(), \"\\n\")\n",
    "\n",
    "    return behaviors_df, history_df, articles_df\n",
    "\n",
    "# Load data\n",
    "behaviors_df, history_df, articles_df = load_data(data_version, data_type, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from parquet files\n",
    "def load_data(version, data_type, print_info=False):\n",
    "    base_path = f\"./data_processed/{version}_{data_type}_\"\n",
    "    files = [\"behaviors_df_expanded.parquet\", \"history_df_expanded.parquet\", \"articles_df_expanded.parquet\", \"users_df_expanded.parquet\"]\n",
    "    dataframes = [pd.read_parquet(f\"{base_path}{file}\") for file in files]\n",
    "    \n",
    "    if print_info:\n",
    "        for df in dataframes:\n",
    "            print(df.info(), \"\\n\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "# Load users_df\n",
    "_, _, _, users_df = load_data(data_version, data_type, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'article_ids_clicked' has more than 1 element\n",
    "behaviors_df = behaviors_df[behaviors_df['article_ids_clicked'].apply(lambda x: len(x) <= 1)]\n",
    "\n",
    "# Convert the list to an integer value\n",
    "behaviors_df['article_ids_clicked'] = behaviors_df['article_ids_clicked'].apply(lambda x: x[0] if x else None).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge history_df into behaviors_df\n",
    "df = behaviors_df.merge(\n",
    "    history_df,\n",
    "    how='inner',\n",
    "    left_on=['user_id'],\n",
    "    right_on=['user_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge articles_df into df\n",
    "df = df.merge(\n",
    "    articles_df.add_prefix('clicked_article_'),\n",
    "    how='inner',\n",
    "    left_on=['article_ids_clicked'],\n",
    "    right_on=['clicked_article_article_id']\n",
    ")\n",
    "\n",
    "# Drop 'clicked_article_article_id' column\n",
    "df = df.drop(columns=['clicked_article_article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge users_df into df\n",
    "df = df.merge(\n",
    "    users_df,\n",
    "    how='inner',\n",
    "    left_on=['user_id'],\n",
    "    right_on=['user_id']\n",
    ")\n",
    "\n",
    "print(df.iloc[1000].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series mapping article_ids_clicked to topics for quick lookup\n",
    "article_topics_map = df.set_index('article_ids_clicked')['clicked_article_topics'].to_dict()\n",
    "\n",
    "# Precompute the impression times window for each article\n",
    "impressions_df = df[['impression_time', 'article_ids_clicked']]\n",
    "\n",
    "# Create a dictionary to store trendiness scores\n",
    "trendiness_scores = {}\n",
    "\n",
    "# Use tqdm to wrap the outer loop for progress tracking\n",
    "for article_ids_clicked, group in tqdm(impressions_df.groupby('article_ids_clicked'), desc=\"Calculating Trendiness\"):\n",
    "    # Get the topics for the current article\n",
    "    topics = article_topics_map.get(article_ids_clicked, [])\n",
    "    \n",
    "    # Calculate trendiness for each impression time in the group\n",
    "    for impression_time in group['impression_time']:\n",
    "        start_time = impression_time - pd.Timedelta(days=7)\n",
    "        \n",
    "        # Filter impressions in the time window\n",
    "        relevant_impressions = impressions_df[\n",
    "            (impressions_df['impression_time'] >= start_time) &\n",
    "            (impressions_df['impression_time'] < impression_time)\n",
    "        ]\n",
    "        \n",
    "        # Calculate trendiness by checking topic overlap\n",
    "        trendiness = relevant_impressions['article_ids_clicked'].apply(\n",
    "            lambda x: any(topic in article_topics_map.get(x, []) for topic in topics)\n",
    "        ).sum()\n",
    "        \n",
    "        # Store the trendiness score\n",
    "        trendiness_scores[(article_ids_clicked, impression_time)] = trendiness\n",
    "\n",
    "# Map the trendiness scores back to the df\n",
    "df['trendiness'] = df.apply(\n",
    "    lambda row: trendiness_scores.get((row['article_ids_clicked'], row['impression_time']), 0), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.path.join('data_processed', f'DATA1.parquet')\n",
    "df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = os.path.join('data_processed', 'DATA1.parquet')\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['user_id', 'article_ids_clicked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the impression times window for each article\n",
    "impressions_df = df[['user_id', 'impression_time', 'article_ids_inview']]\n",
    "\n",
    "# Convert 'article_ids_inview' and 'article_ids_clicked' to lists explicitly\n",
    "def convert_to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif isinstance(x, str):\n",
    "        return x.strip('[]').replace(\"'\", \"\").split(', ')\n",
    "    elif isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "df['article_ids_inview'] = df['article_ids_inview'].apply(convert_to_list)\n",
    "\n",
    "# seen_before_clicked calculation function\n",
    "def calculate_seen_before_clicked(user_id, article_id, impression_time, hours=48):\n",
    "    start_time = impression_time - pd.Timedelta(hours=hours)\n",
    "    relevant_impressions = impressions_df[\n",
    "        (impressions_df['user_id'] == user_id) &\n",
    "        (impressions_df['impression_time'] >= start_time) & \n",
    "        (impressions_df['impression_time'] < impression_time)\n",
    "    ]\n",
    "    seen_before_clicked = relevant_impressions['article_ids_inview'].apply(lambda x: article_id in x).sum()\n",
    "    return seen_before_clicked\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "df['seen_before_clicked'] = df.progress_apply(\n",
    "    lambda row: calculate_seen_before_clicked(row['user_id'], row['article_id'], row['impression_time']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['user_id', 'article_ids_clicked', 'seen_before_clicked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article Delay\n",
    "df['article_delay'] = (df['impression_time'] - df['clicked_article_published_time']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['impression_hour'] = df['impression_time'].dt.hour\n",
    "df['impression_day_of_week'] = df['impression_time'].dt.dayofweek\n",
    "\n",
    "# Convert time of day and day of week to cyclical features\n",
    "df['impression_hour_sin'] = np.sin(2 * np.pi * df['impression_hour'] / 24)\n",
    "df['impression_hour_cos'] = np.cos(2 * np.pi * df['impression_hour'] / 24)\n",
    "df['impression_day_of_week_sin'] = np.sin(2 * np.pi * df['impression_day_of_week'] / 7)\n",
    "df['impression_day_of_week_cos'] = np.cos(2 * np.pi * df['impression_day_of_week'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_clusters(article_ids, articles_df):\n",
    "    # Filter the articles_df to get NER clusters for the given article IDs\n",
    "    clusters = articles_df[articles_df['article_id'].isin(article_ids)]['ner_clusters']\n",
    "    # Flatten the list of lists and join into a single string\n",
    "    return ' '.join([item for sublist in clusters for item in sublist])\n",
    "\n",
    "# Extract NER clusters for each article_id_fixed\n",
    "df['article_ner_clusters'] = df['article_id_fixed'].apply(lambda ids: get_ner_clusters(ids, articles_df))\n",
    "\n",
    "# Convert the clicked_article_ner_clusters to strings\n",
    "df['clicked_article_ner_clusters_str'] = df['clicked_article_ner_clusters'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Combine all NER cluster strings for TF-IDF computation\n",
    "corpus = pd.concat([df['clicked_article_ner_clusters_str'], df['article_ner_clusters']])\n",
    "\n",
    "# Create a TF-IDF vectoriser and fit it to the corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calculate cosine similarity between clicked NER clusters and article NER clusters\n",
    "def calculate_similarity(row_index):\n",
    "    # Compute similarity between the clicked NER and article NER for each row\n",
    "    return cosine_similarity(tfidf_matrix[row_index], tfidf_matrix[row_index + len(df)])[0][0]\n",
    "\n",
    "# Apply similarity calculation for each row\n",
    "df['ner_similarity'] = [calculate_similarity(i) for i in range(len(df))]\n",
    "\n",
    "# Display the results\n",
    "df[['article_ids_clicked', 'ner_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.path.join('data_processed', f'DATA2.parquet')\n",
    "df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = os.path.join('data_processed', 'DATA2.parquet')\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure 'article_ids_inview' is a list\n",
    "df['article_ids_inview'] = df['article_ids_inview'].apply(lambda x: list(x))\n",
    "\n",
    "# Generate binary labels\n",
    "def generate_binary_labels(row):\n",
    "    return [1 if article_id == row['article_ids_clicked'] else 0 for article_id in row['article_ids_inview']]\n",
    "\n",
    "df['binary_labels'] = df.apply(generate_binary_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Define the columns to extract\n",
    "columns_to_extract = [\n",
    "    'article_ids_inview',\n",
    "    'trendiness',\n",
    "    'seen_before_clicked',\n",
    "    'article_delay',\n",
    "    'impression_hour_sin',\n",
    "    'impression_hour_cos',\n",
    "    'impression_day_of_week_sin',\n",
    "    'impression_day_of_week_cos',\n",
    "    'ner_similarity',\n",
    "    'clicked_article_sentiment_score',\n",
    "    'binary_labels'\n",
    "]\n",
    "\n",
    "# Create the new dataframe with the specified columns\n",
    "df = df[columns_to_extract]\n",
    "\n",
    "# Separate the target from the features\n",
    "X = df.drop('binary_labels', axis=1)\n",
    "y = df['binary_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Flatten the data\n",
    "X_inview = np.concatenate(df['article_ids_inview'].values)\n",
    "y = np.concatenate(df['binary_labels'].values)\n",
    "\n",
    "# Repeat other features for each article in 'article_ids_inview'\n",
    "features = [\n",
    "    'trendiness', 'seen_before_clicked', 'article_delay', \n",
    "    'impression_hour_sin', 'impression_hour_cos', \n",
    "    'impression_day_of_week_sin', 'impression_day_of_week_cos', \n",
    "    'ner_similarity', 'clicked_article_sentiment_score'\n",
    "]\n",
    "\n",
    "X_other_features = np.repeat(df[features].values, df['article_ids_inview'].apply(len).values, axis=0)\n",
    "\n",
    "# Combine 'article_ids_inview' with other features\n",
    "X_combined = np.column_stack((X_inview, X_other_features))\n",
    "\n",
    "# Standardize the features (excluding 'article_ids_inview')\n",
    "scaler = StandardScaler()\n",
    "X_combined[:, 1:] = scaler.fit_transform(X_combined[:, 1:])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "max_article_id = max(np.concatenate(df['article_ids_inview'].values))\n",
    "print(f\"Maximum article ID: {max_article_id}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
    "\n",
    "# Define the model\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "embedding_layer = Embedding(input_dim=max_article_id + 1, output_dim=8, input_length=1)(input_layer[:, 0])\n",
    "embedding_layer = Flatten()(embedding_layer)\n",
    "concatenated = Concatenate()([embedding_layer, input_layer[:, 1:]])\n",
    "\n",
    "x = Dense(128, activation='relu')(concatenated)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "# Fit the model with class weights\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
