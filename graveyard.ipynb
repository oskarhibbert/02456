{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    corr_matrix = pd.DataFrame(df, columns=df.columns).corr()\n",
    "    mask = (corr_matrix < -0.1) | (corr_matrix > 0.1)\n",
    "\n",
    "    plt.figure(figsize=(24, 18))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap=\"coolwarm\",\n",
    "        mask=~mask,\n",
    "        cbar_kws={\"label\": \"Correlation Coefficient\"},\n",
    "    )\n",
    "    plt.title(\"Filtered Feature Correlation Matrix (|Corr| > 0.1)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "plot_correlation_matrix(behaviors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns_to_drop):\n",
    "    df = df.drop(columns=columns_to_drop, errors=\"raise\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"impression_id\",\n",
    "    \"impression_time\",\n",
    "    \"article_ids_clicked\",\n",
    "    \"impression_day_of_week\",\n",
    "    \"impression_hour\",\n",
    "]\n",
    "\n",
    "behaviors_df = drop_columns(behaviors_df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_features_and_target(df):\n",
    "    y = df['target']\n",
    "    X = df.drop(columns=['target'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = define_features_and_target(behaviors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_information(X, y):\n",
    "    mutual_info = mutual_info_classif(X, y, random_state=42)\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {\"Feature\": X.columns, \"Importance\": mutual_info}\n",
    "    ).sort_values(by=\"Importance\", ascending=False, inplace=False)\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "mutual_information = compute_mutual_information(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mutual_information(feature_importance):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance, color=\"blue\")\n",
    "    plt.title(\"Feature Importance Based on Mutual Information\")\n",
    "\n",
    "    for index, value in enumerate(feature_importance[\"Importance\"]):\n",
    "        plt.text(value, index + 0.1, f\"  {value:.4f}\", va=\"center\", ha=\"left\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_mutual_information(mutual_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "X_scaled = scale_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X_scaled, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_random_forest_importance(X_train, y_train):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    random_forest_importance = pd.DataFrame(\n",
    "        {\"Feature\": X.columns, \"Importance\": rf.feature_importances_}\n",
    "    ).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return random_forest_importance\n",
    "\n",
    "random_forest_importance = compute_random_forest_importance(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_forest_importance(random_forest_importance):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(\n",
    "        x=\"Importance\", y=\"Feature\", data=random_forest_importance, color=\"green\"\n",
    "    )\n",
    "    plt.title(\"Random Forest Feature Importances\")\n",
    "\n",
    "    for index, value in enumerate(random_forest_importance[\"Importance\"]):\n",
    "        plt.text(value, index + 0.1, f\"  {value:.4f}\", va=\"center\", ha=\"left\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_random_forest_importance(random_forest_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rfe(X_train, y_train, n_features=5):\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "    rfe = RFE(estimator=log_reg, n_features_to_select=n_features)\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    rfe_ranking = pd.DataFrame({\"Feature\": X.columns, \"Ranking\": rfe.ranking_})\n",
    "    rfe_ranking = rfe_ranking.sort_values(by=\"Ranking\")\n",
    "\n",
    "    print(\"RFE Feature Ranking:\")\n",
    "    print(rfe_ranking)\n",
    "\n",
    "    return rfe_ranking\n",
    "\n",
    "rfe_ranking = perform_rfe(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_device_type(df):\n",
    "    \"\"\"\n",
    "    One-hot encode the 'device_type' column.\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(df, columns=['device_type'])\n",
    "\n",
    "def categorize_time_features(df):\n",
    "    \"\"\"\n",
    "    Convert time-based features (impression_hour and impression_day) into categories.\n",
    "    \"\"\"\n",
    "    # Convert impression_hour into categories: night, morning, afternoon, evening\n",
    "    df['impression_hour'] = pd.cut(\n",
    "        df['impression_hour'], \n",
    "        bins=[0, 5, 11, 17, 23], \n",
    "        labels=['night', 'morning', 'afternoon', 'evening']\n",
    "    )\n",
    "\n",
    "    # Convert impression_day into categories: beginning, middle, end\n",
    "    df['impression_day'] = pd.cut(\n",
    "        df['impression_day'], \n",
    "        bins=[0, 9, 19, 31], \n",
    "        labels=['beginning', 'middle', 'end']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def one_hot_encode_time_features(df):\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding to time features and additional categorical columns.\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(\n",
    "        df, \n",
    "        columns=['impression_year', 'impression_month', 'impression_day', \n",
    "                 'impression_day_of_week', 'impression_hour']\n",
    "    )\n",
    "\n",
    "# Workflow\n",
    "datasets[\"train/behaviors\"]_exploded_encoded = encode_device_type(datasets[\"train/behaviors\"]_exploded)\n",
    "datasets[\"train/behaviors\"]_exploded_encoded = categorize_time_features(datasets[\"train/behaviors\"]_exploded_encoded)\n",
    "datasets[\"train/behaviors\"]_exploded_encoded = one_hot_encode_time_features(datasets[\"train/behaviors\"]_exploded_encoded)\n",
    "\n",
    "# Display Results\n",
    "print(list(datasets[\"train/behaviors\"]_exploded_encoded.columns), '')\n",
    "print(datasets[\"train/behaviors\"]_exploded_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_is_sso_and_subscriber(df):\n",
    "    \"\"\"\n",
    "    Merge 'is_sso_user' and 'is_subscriber' into a single feature.\n",
    "    The new feature will indicate if the user is either an SSO user or a subscriber.\n",
    "    \"\"\"\n",
    "    df['is_sso_or_subscriber'] = df['is_sso_user'] | df['is_subscriber']\n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "datasets[\"train/behaviors\"]_exploded = merge_is_sso_and_subscriber(datasets[\"train/behaviors\"]_exploded)\n",
    "\n",
    "# Verify the new feature\n",
    "print(datasets[\"train/behaviors\"]_exploded[['is_sso_user', 'is_subscriber', 'is_sso_or_subscriber']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[\"train/behaviors\"]_exploded['is_sso_user'].unique())\n",
    "print(datasets[\"train/behaviors\"]_exploded['is_subscriber'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[\"train/behaviors\"]_exploded.corr()['is_sso_or_subscriber'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append('is_sso_or_subscriber')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train/behaviors\"]_exploded = datasets[\"train/behaviors\"]_exploded.drop(columns=['is_sso_user', 'is_subscriber'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate user history metrics\n",
    "def compute_user_metrics(datasets[\"train/behaviors\"]):\n",
    "    \"\"\"\n",
    "    Compute user history metrics such as total interactions, average read time,\n",
    "    most active day, most used device type, and toporite term.\n",
    "    \"\"\"\n",
    "    user_metrics = datasets[\"train/behaviors\"].groupby('user_id').agg(\n",
    "        total_interactions=('impression_id', 'count'),  # Total number of interactions\n",
    "        avg_read_time=('read_time', 'mean'),           # Average read time\n",
    "        avg_scroll_percentage=('scroll_percentage', 'mean'),  # Avg scroll percentage\n",
    "        most_active_day=('impression_day_of_week', lambda x: x.mode()[0] if not x.empty else None),  # Most common day\n",
    "        most_used_device=('device_type', lambda x: x.mode()[0] if not x.empty else None),            # Most common device\n",
    "        toporite_term=('article_ids_clicked', lambda x: x.mode()[0] if not x.empty else None)  # Most frequently clicked article/term\n",
    "    ).reset_index()\n",
    "    \n",
    "    return user_metrics\n",
    "\n",
    "# Step 2: Merge computed user metrics back to the main behaviors dataframe\n",
    "def merge_user_metrics(datasets[\"train/behaviors\"], user_metrics):\n",
    "    \"\"\"\n",
    "    Merge computed user-level metrics back into the main dataframe.\n",
    "    \"\"\"\n",
    "    merged_df = datasets[\"train/behaviors\"].merge(user_metrics, on='user_id', how='left')\n",
    "    return merged_df\n",
    "\n",
    "# Step 3: Apply the functions to compute and merge metrics\n",
    "user_metrics_df = compute_user_metrics(datasets[\"train/behaviors\"])  # Compute metrics\n",
    "datasets[\"train/behaviors\"]_exploded = merge_user_metrics(datasets[\"train/behaviors\"]_exploded, user_metrics_df)  # Merge metrics\n",
    "\n",
    "# Step 4: Verify the merged dataframe\n",
    "print(datasets[\"train/behaviors\"]_exploded[['user_id', 'total_interactions', 'avg_read_time', \n",
    "                             'most_active_day', 'most_used_device', 'toporite_term']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_metrics_df.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
